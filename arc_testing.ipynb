{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# For LLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dictionary(data):\n",
    "    \"\"\"\n",
    "    Splits the tasks that have multiple test input/output pairs into separate entries.\n",
    "\n",
    "    Args:\n",
    "    data (dict): The original dictionary containing tasks with 'test' and 'train' fields.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - result (dict): The dictionary with tasks split into separate entries if they have multiple test pairs.\n",
    "        - split_files (list): A list of keys for the tasks that were split.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    split_files = []\n",
    "    for key, value in data.items():\n",
    "        test_list = value.get(\"test\", [])\n",
    "        train_list = value.get(\"train\", [])\n",
    "        if len(test_list) > 1:\n",
    "            for idx, test_item in enumerate(test_list):\n",
    "                new_key = f\"{key}_{idx}\"\n",
    "                result[new_key] = {\n",
    "                    \"test\": [test_item],\n",
    "                    \"train\": train_list\n",
    "                }\n",
    "                split_files.append(new_key)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result, split_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files split: 19\n",
      "File names:\n",
      "12997ef3_0\n",
      "12997ef3_1\n",
      "1d398264_0\n",
      "1d398264_1\n",
      "31d5ba1a_0\n",
      "31d5ba1a_1\n",
      "3b4c2228_0\n",
      "3b4c2228_1\n",
      "4852f2fa_0\n",
      "4852f2fa_1\n",
      "4c177718_0\n",
      "4c177718_1\n",
      "5d2a5c43_0\n",
      "5d2a5c43_1\n",
      "6ea4a07e_0\n",
      "6ea4a07e_1\n",
      "8b28cd80_0\n",
      "8b28cd80_1\n",
      "9110e3c5_0\n",
      "9110e3c5_1\n",
      "9b4c17c4_0\n",
      "9b4c17c4_1\n",
      "b1fc8b8e_0\n",
      "b1fc8b8e_1\n",
      "bbb1b8b6_0\n",
      "bbb1b8b6_1\n",
      "c074846d_0\n",
      "c074846d_1\n",
      "d5c634a2_0\n",
      "d5c634a2_1\n",
      "da2b0fe3_0\n",
      "da2b0fe3_1\n",
      "e21a174a_0\n",
      "e21a174a_1\n",
      "e345f17b_0\n",
      "e345f17b_1\n",
      "f3e62deb_0\n",
      "f3e62deb_1\n",
      "    file_name                                              train  \\\n",
      "0    00576224  [{'input': [[8, 6], [6, 4]], 'output': [[8, 6,...   \n",
      "1    009d5c81  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2    00dbd492  [{'input': [[2, 2, 2, 2, 2, 0, 0], [2, 0, 0, 0...   \n",
      "3    03560426  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...   \n",
      "4    05a7bcf2  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "..        ...                                                ...   \n",
      "414  fd096ab6  [{'input': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "415  fd4b2b02  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "416  fe9372f3  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...   \n",
      "417  fea12743  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],...   \n",
      "418  ff72ca3e  [{'input': [[0, 0, 0, 0, 0, 0, 5, 0, 0, 0], [0...   \n",
      "\n",
      "                                            test_input  \\\n",
      "0                        [{'input': [[3, 2], [7, 8]]}]   \n",
      "1    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "2    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...   \n",
      "4    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...   \n",
      "..                                                 ...   \n",
      "414  [{'input': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "415  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "416  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "417  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],...   \n",
      "418  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                                           test_output  \\\n",
      "0    [{'output': [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, ...   \n",
      "1    [{'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "2    [{'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "3    [{'output': [[7, 0, 0, 0, 0, 0, 0, 0, 0, 0], [...   \n",
      "4    [{'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,...   \n",
      "..                                                 ...   \n",
      "414  [{'output': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
      "415  [{'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "416  [{'output': [[0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0,...   \n",
      "417  [{'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...   \n",
      "418  [{'output': [[0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...   \n",
      "\n",
      "                                                  test  \n",
      "0    [{'input': [[3, 2], [7, 8]], 'output': [[3, 2,...  \n",
      "1    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...  \n",
      "4    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
      "..                                                 ...  \n",
      "414  [{'input': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "415  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "416  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "417  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],...  \n",
      "418  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[419 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compare results to solution\n",
    "test_run = True\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "# Load JSON data from the files\n",
    "if test_run:\n",
    "    with open('data/arc-agi_evaluation_challenges.json') as f:\n",
    "        challenges = json.load(f)\n",
    "        # Split tasks with multiple test inputs\n",
    "        challenges, split_files = split_dictionary(challenges) \n",
    "\n",
    "    with open('data/arc-agi_evaluation_solutions.json') as f:\n",
    "        solutions = json.load(f)\n",
    "else:\n",
    "    with open('data/arc-agi_test_challenges.json') as f:\n",
    "        challenges = json.load(f)\n",
    "    # Split tasks with multiple test inputs\n",
    "    challenges, split_files = split_dictionary(challenges) \n",
    "\n",
    "# Print how many files have been split and their names\n",
    "split_file_count = len(split_files)//2\n",
    "\n",
    "print(f\"Number of files split: {split_file_count}\")\n",
    "print(\"File names:\")\n",
    "for name in split_files:\n",
    "    print(name)\n",
    "\n",
    "# Prepare data\n",
    "data = []\n",
    "        \n",
    "for file_name, grids in challenges.items():\n",
    "    train_grids = grids.get('train', [])\n",
    "    test_inputs = grids.get('test', [])\n",
    "    if test_run:\n",
    "        # Handle files with multiple test inputs\n",
    "        parts = file_name.split('_')\n",
    "        if len(parts) > 1:\n",
    "            test_nr = int(parts[1])\n",
    "        else:\n",
    "            test_nr = 0\n",
    "        test_outputs = solutions.get(parts[0], [])\n",
    "        # Transform test grids to lists of dicts with 'output' key\n",
    "        test_outputs_transformed = [{'output': test_outputs[test_nr]}]\n",
    "        # Combine test inputs and outputs in alternating manner\n",
    "        combined_tests = [{'input': test_inputs[0]['input'], 'output': test_outputs_transformed[0]['output']}]\n",
    "    data.append({\n",
    "            'file_name': file_name,\n",
    "            'train': train_grids,\n",
    "            'test_input': test_inputs,\n",
    "            'test_output': test_outputs_transformed if test_run else [[0, 0]],\n",
    "            'test': combined_tests if test_run else test_inputs\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Prepare model, tokenizer: 0.981 sec.\n"
     ]
    }
   ],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\"\n",
    "\n",
    "# Set the data type for computations to float16, bfloat16 not supported on T4/P100\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Configure the BitsAndBytes settings for 4-bit quantization to reduce memory usage\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # Enable 4-bit quantization\n",
    "#     bnb_4bit_use_double_quant=True,  # Use double quantization for improved precision\n",
    "#     bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,  # Set the computation data type\n",
    "# )\n",
    "\n",
    "# Specify the model ID for loading the fine-tuned Llama 3 model\n",
    "model_id = \"models/llama3.2_1B/\"\n",
    "\n",
    "# Configure the BitsAndBytes settings for 8-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True  # Enable 8-bit quantization\n",
    ")\n",
    "\n",
    "# Record the start time to measure the loading duratio\n",
    "time_start = time()\n",
    "print(\"Loading model\")\n",
    "\n",
    "# Load the pre-trained model with specified configurations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, # Allow the model to use custom code from the repository\n",
    "    #quantization_config=bnb_config, # Apply the 4-bit or 8-bit quantization configuration\n",
    "    #attn_implementation='sdpa', # Use scaled-dot product attention for better performance\n",
    "    torch_dtype=compute_dtype, # Set the data type for the model\n",
    "    use_cache=False, # Disable caching to save memory\n",
    "    device_map= 'auto' #{\"\": 0}, # Automatically map the model to available devices (e.g., GPUs)\n",
    ")\n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE # Apply the chat message template\n",
    "\n",
    "# Record the end time and print the duration for preparing the model and tokenizer\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 419/419 [00:00<00:00, 2266.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'text', 'solution'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\n",
    "system_prompt = '''You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.'''\n",
    "\n",
    "# User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\n",
    "user_message_template = '''Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
    "----------------------------------------\n",
    "{training_data}\n",
    "----------------------------------------\n",
    "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
    "----------------------------------------\n",
    "[{{'input': {input_test_data}, 'output': [[]]}}]\n",
    "----------------------------------------\n",
    "What is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:'''\n",
    "\n",
    "def preprocess(task, test_run, train_mode=False):\n",
    "    \"\"\"\n",
    "    Preprocess a single ARC task to create the prompt and solution for the model.\n",
    "\n",
    "    This function formats the system and user messages using a predefined template and the task's training and test data.\n",
    "    If in training mode, it also includes the assistant's message with the expected output.\n",
    "\n",
    "    Parameters:\n",
    "    task (dict): The ARC task data containing training and test examples.\n",
    "    train_mode (bool): If True, includes the assistant's message with the expected output for training purposes.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the formatted text prompt, the solution, and the file name.\n",
    "    \"\"\"\n",
    "    # System message\n",
    "    system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
    "\n",
    "    # Extract training data and input grid from the task\n",
    "    training_data = task['train']\n",
    "    input_test_data = task['test'][0]['input']\n",
    "    if test_run:\n",
    "        output_test_data = task['test'][0]['output']\n",
    "    else:\n",
    "        output_test_data = [[0 ,0]]\n",
    "\n",
    "    # Format the user message with training data and input test data\n",
    "    user_message_content = user_message_template.format(training_data=training_data, input_test_data=input_test_data)\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content\n",
    "    }\n",
    "\n",
    "    # Include the assistant message with the expected output if in training mode\n",
    "    if train_mode:\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": str(output_test_data)\n",
    "        }\n",
    "\n",
    "        # Combine system, user, and assistant messages\n",
    "        messages = [system_message, user_message, assistant_message]\n",
    "    else:\n",
    "        messages = [system_message, user_message]\n",
    "    # Convert messages using the chat template for use with the instruction finetuned version of Llama\n",
    "    messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    if test_run:\n",
    "        return {\"text\": messages, \"solution\": output_test_data, \"file_name\": task['file_name']}\n",
    "    else:\n",
    "        return {\"text\": messages, \"file_name\": task['file_name']}\n",
    "\n",
    "# Convert the loaded data to a Huggingface Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Apply the preprocess function to each task in the dataset\n",
    "dataset = dataset.map(lambda x: preprocess(x, test_run), batched=False, remove_columns=dataset.column_names)\n",
    "dataset = dataset.select([0, 32])\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2/2 [00:00<00:00, 176.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tasks contain too many tokens if we set max_tokens to 8000\n",
      "The dataset contains 1 tasks to evaluate the model\n",
      "Index(['file_name', 'text', 'solution'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum number of tokens allowed\n",
    "max_tokens = 8000  # Adjust this value as needed\n",
    "\n",
    "\n",
    "# Function to calculate the number of tokens\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a given text using the tokenizer.\n",
    "\n",
    "    This function uses the tokenizer to encode the input text and returns the\n",
    "    number of tokens. It is useful for ensuring that the text length stays\n",
    "    within the model's context window.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of tokens in the input text.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Filter the dataset to include only tasks with a number of tokens within the allowed limit\n",
    "filtered_dataset = dataset.filter(lambda x: count_tokens(x['text']) <= max_tokens)\n",
    "\n",
    "# Print the number of tasks filtered out and the remaining tasks\n",
    "print(f'{len(dataset)-len(filtered_dataset)} tasks contain too many tokens if we set max_tokens to {max_tokens}')\n",
    "print(f'The dataset contains {len(filtered_dataset)} tasks to evaluate the model')\n",
    "\n",
    "print(filtered_dataset.to_pandas().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define your LLM pipeline\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "text_gen_pipeline.tokenizer.pad_token_id = text_gen_pipeline.model.config.eos_token_id\n",
    "\n",
    "# Define terminators for the pipeline\n",
    "terminators = [\n",
    "    text_gen_pipeline.tokenizer.eos_token_id,\n",
    "    text_gen_pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Function to generate outputs\n",
    "def generate_solution(task, max_new_tokens=512, do_sample=True, temperature=0.1, top_p=0.1):\n",
    "    \"\"\"\n",
    "    Generate a solution for an ARC task using the language model.\n",
    "\n",
    "    This function takes a task prompt, generates a solution using the text generation pipeline,\n",
    "    and extracts the generated solution from the model's output.\n",
    "\n",
    "    Parameters:\n",
    "    task (dict): The ARC task data containing the prompt and other relevant information.\n",
    "    max_new_tokens (int, optional): The maximum number of new tokens to generate. Default is 512.\n",
    "    do_sample (bool, optional): Whether to use sampling; if False, greedy decoding is used. Default is True.\n",
    "    temperature (float, optional): The sampling temperature. Lower values make the model more conservative. Default is 0.1.\n",
    "    top_p (float, optional): The cumulative probability for nucleus sampling. Lower values make the model more conservative. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the generated solution.\n",
    "    \"\"\"\n",
    "    # Extract the prompt from the task\n",
    "    prompt = task['text']\n",
    "    \n",
    "    # Generate the model's output based on the prompt\n",
    "    outputs = text_gen_pipeline(\n",
    "        prompt, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        eos_token_id=terminators, \n",
    "        do_sample=do_sample, \n",
    "        temperature=temperature, \n",
    "        top_p=top_p\n",
    "    )\n",
    "    \n",
    "    # Extract the generated solution from the model's output\n",
    "    generated_solutions = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    return {'generated_solution': generated_solutions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(text):\n",
    "    \"\"\"\n",
    "    Extract the solution array from the generated text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text containing the generated solution.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists representing the extracted solution array.\n",
    "          Returns [[0]] if no valid solution is found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the part of the text that looks like a nested list\n",
    "        start = text.index('[[')\n",
    "        end = text.index(']]', start) + 2\n",
    "        array_str = text[start:end]\n",
    "        \n",
    "        # Use ast.literal_eval to safely evaluate the string as a Python expression\n",
    "        array = ast.literal_eval(array_str)\n",
    "        \n",
    "        # Check if the result is a list of lists\n",
    "        if all(isinstance(i, list) for i in array):\n",
    "            return array\n",
    "        else:\n",
    "            return [[0]]\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [[0]]\n",
    "\n",
    "def pad_array_with_value(array, target_shape, pad_value):\n",
    "    \"\"\"\n",
    "    Pad the given array to the target shape with the specified pad value.\n",
    "\n",
    "    This function pads the original array to fit the target shape by adding additional\n",
    "    pixels at the ends. This method ensures that the smaller array is placed at the\n",
    "    top-left corner of the target shape, making sense of the number of correct pixels\n",
    "    during comparison.\n",
    "\n",
    "    Note:\n",
    "    Depending on how you pad the arrays, the number of correct pixels might vary.\n",
    "    For example, placing the smaller array in the center versus adding pixels at the ends\n",
    "    can yield different results. Here, we pad by adding pixels at the ends.\n",
    "\n",
    "    Parameters:\n",
    "    array (list): The original array to be padded.\n",
    "    target_shape (tuple): The desired shape of the padded array (rows, columns).\n",
    "    pad_value (int): The value to use for padding the array.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A padded array with the specified target shape and pad value.\n",
    "    \"\"\"\n",
    "    padded_array = np.full(target_shape, pad_value, dtype=int)\n",
    "    original_shape = np.array(array).shape\n",
    "    padded_array[:original_shape[0], :original_shape[1]] = array\n",
    "    return padded_array\n",
    "\n",
    "def compare_solutions_with_padding(generated_output, correct_output, pad_value=-1):\n",
    "    \"\"\"\n",
    "    Compare the generated output with the correct output, using padding to align their shapes.\n",
    "\n",
    "    Parameters:\n",
    "    generated_output (list): The generated solution array.\n",
    "    correct_output (list): The correct solution array.\n",
    "    pad_value (int, optional): The value to use for padding. Default is -1. The colour value -1 should not be present in the solutions.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - is_correct (bool): True if the solutions match exactly, False otherwise.\n",
    "        - correct_percentage (float): The percentage of correctly matched pixels.\n",
    "    \"\"\"\n",
    "    max_rows = max(len(generated_output), len(correct_output))\n",
    "    max_cols = max(len(generated_output[0]), len(correct_output[0]))\n",
    "    target_shape = (max_rows, max_cols)\n",
    "    \n",
    "    padded_generated = pad_array_with_value(generated_output, target_shape, pad_value)\n",
    "    padded_correct = pad_array_with_value(correct_output, target_shape, pad_value)\n",
    "    \n",
    "    total_pixels = max_rows * max_cols\n",
    "    correct_pixels = np.sum((padded_generated == padded_correct) & (padded_generated != pad_value) & (padded_correct != pad_value))\n",
    "    correct_percentage = (correct_pixels / total_pixels) * 100\n",
    "    \n",
    "    is_correct = (correct_pixels == total_pixels)\n",
    "    \n",
    "    return is_correct, correct_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solutions_passk_batch(task, k=3, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate k solutions for an ARC task using batch generation for better efficiency.\n",
    "\n",
    "    Parameters:\n",
    "    task (dict): The ARC task data containing the prompt and other relevant information.\n",
    "    k (int): Number of solutions to generate for pass@k evaluation. Default is 3.\n",
    "    max_new_tokens (int, optional): The maximum number of new tokens to generate. Default is 512.\n",
    "    do_sample (bool, optional): Whether to use sampling; if False, greedy decoding is used. Default is True.\n",
    "    temperature (float, optional): The sampling temperature. Higher values for more diversity. Default is 0.7.\n",
    "    top_p (float, optional): The cumulative probability for nucleus sampling. Default is 0.9.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing k generated solutions.\n",
    "    \"\"\"\n",
    "    # Extract the prompt from the task\n",
    "    prompt = task['text']\n",
    "    \n",
    "    # Create k copies of the prompt for batch generation\n",
    "    prompts = [prompt] * k\n",
    "    \n",
    "    # Generate k solutions in batch\n",
    "    outputs = text_gen_pipeline(\n",
    "        prompts,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        batch_size=k\n",
    "    )\n",
    "    \n",
    "    # Extract generated solutions\n",
    "    generated_solutions = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        \n",
    "        generated_solution = output[\"generated_text\"][len(prompt):]\n",
    "        generated_solutions.append(generated_solution)\n",
    "    \n",
    "    return {f'generated_solution_{i+1}': sol for i, sol in enumerate(generated_solutions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_passk_solutions(task, k=3):\n",
    "    \"\"\"\n",
    "    Evaluate pass@k performance by checking if at least one of k solutions is correct.\n",
    "\n",
    "    Parameters:\n",
    "    task (dict): The task containing generated solutions and ground truth.\n",
    "    k (int): Number of solutions to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    dict: Evaluation results including pass@k success, best accuracy, and solution details.\n",
    "    \"\"\"\n",
    "    if not test_run:\n",
    "        # If not in test mode, just extract solutions without evaluation\n",
    "        extracted_solutions = []\n",
    "        for i in range(1, k+1):\n",
    "            solution_key = f'generated_solution_{i}'\n",
    "            if solution_key in task:\n",
    "                gen_solution = extract_solution(task[solution_key])\n",
    "                extracted_solutions.append(gen_solution)\n",
    "        \n",
    "        return {\n",
    "            'extracted_solutions': extracted_solutions,\n",
    "            'pass_at_k': False,  # Cannot evaluate without ground truth\n",
    "            'best_accuracy': 0.0,\n",
    "            'best_solution_idx': -1\n",
    "        }\n",
    "    \n",
    "    true_solution = task['solution']\n",
    "    file_name = task['file_name']\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    best_solution_idx = -1\n",
    "    best_solution = None\n",
    "    is_any_correct = False\n",
    "    \n",
    "    solution_results = []\n",
    "    \n",
    "    # Evaluate each of the k solutions\n",
    "    for i in range(1, k+1):\n",
    "        solution_key = f'generated_solution_{i}'\n",
    "        if solution_key in task:\n",
    "            generated_text = task[solution_key]\n",
    "            gen_solution = extract_solution(generated_text)\n",
    "\n",
    "            if not is_rectangular(gen_solution) or not is_rectangular(true_solution):\n",
    "                print(f\"Skipping {file_name} due to jagged array.\")\n",
    "                continue\n",
    "\n",
    "            # Compare with ground truth\n",
    "            is_correct, correct_percentage = compare_solutions_with_padding(gen_solution, true_solution)\n",
    "            \n",
    "            solution_results.append({\n",
    "                'solution_idx': i,\n",
    "                'is_correct': is_correct,\n",
    "                'accuracy': correct_percentage,\n",
    "                'extracted_solution': gen_solution\n",
    "            })\n",
    "            \n",
    "            # Track the best solution\n",
    "            if correct_percentage > best_accuracy:\n",
    "                best_accuracy = correct_percentage\n",
    "                best_solution_idx = i\n",
    "                best_solution = gen_solution\n",
    "            \n",
    "            # Check if any solution is completely correct\n",
    "            if is_correct:\n",
    "                is_any_correct = True\n",
    "    \n",
    "    return {\n",
    "        'file_name': file_name,\n",
    "        'pass_at_k': is_any_correct,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_solution_idx': best_solution_idx,\n",
    "        'best_solution': best_solution,\n",
    "        'all_solutions': solution_results,\n",
    "        'k': len(solution_results)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_passk_evaluation(filtered_dataset, k=3, use_batch=True):\n",
    "    \"\"\"\n",
    "    Run pass@k evaluation on the filtered dataset.\n",
    "\n",
    "    Parameters:\n",
    "    filtered_dataset: The dataset to evaluate.\n",
    "    k (int): Number of solutions to generate per task.\n",
    "    use_batch (bool): Whether to use batch generation for efficiency.\n",
    "\n",
    "    Returns:\n",
    "    dict: Comprehensive evaluation results.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {k} solutions per task for pass@{k} evaluation...\")\n",
    "    \n",
    "    # Choose generation function #generate_solutions_passk_batch if use_batch else\n",
    "    generation_func = generate_solutions_passk_batch\n",
    "    \n",
    "    # Generate k solutions for each task\n",
    "    dataset_with_solutions = filtered_dataset.map(\n",
    "        lambda x: generation_func(x, k=k), \n",
    "        batched=False\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluating solutions...\")\n",
    "    \n",
    "    # Evaluate pass@k performance\n",
    "    evaluation_results = []\n",
    "    \n",
    "    for i, task in enumerate(dataset_with_solutions):\n",
    "        result = evaluate_passk_solutions(task, k=k)\n",
    "        evaluation_results.append(result)\n",
    "        \n",
    "        if test_run and (i + 1) % 10 == 0:\n",
    "            print(f\"Evaluated {i + 1}/{len(dataset_with_solutions)} tasks\")\n",
    "    \n",
    "    return evaluation_results, dataset_with_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_passk_results(evaluation_results, k):\n",
    "    \"\"\"\n",
    "    Analyze and print pass@k evaluation results.\n",
    "\n",
    "    Parameters:\n",
    "    evaluation_results (list): List of evaluation results from run_passk_evaluation.\n",
    "    k (int): The k value used in pass@k evaluation.\n",
    "    \"\"\"\n",
    "    if not test_run:\n",
    "        print(\"Cannot analyze results - not in test mode (no ground truth available)\")\n",
    "        return\n",
    "    \n",
    "    total_tasks = len(evaluation_results)\n",
    "    pass_at_k_count = sum(1 for result in evaluation_results if result['pass_at_k'])\n",
    "    \n",
    "    # Calculate average best accuracy\n",
    "    avg_best_accuracy = sum(result['best_accuracy'] for result in evaluation_results) / total_tasks\n",
    "    \n",
    "    # Calculate accuracy for each attempt position\n",
    "    attempt_accuracies = {}\n",
    "    for i in range(1, k+1):\n",
    "        accuracies = []\n",
    "        for result in evaluation_results:\n",
    "            for sol_result in result['all_solutions']:\n",
    "                if sol_result['solution_idx'] == i:\n",
    "                    accuracies.append(sol_result['accuracy'])\n",
    "        if accuracies:\n",
    "            attempt_accuracies[f'attempt_{i}'] = sum(accuracies) / len(accuracies)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== Pass@{k} Evaluation Results ===\")\n",
    "    print(f\"Total tasks evaluated: {total_tasks}\")\n",
    "    print(f\"Tasks solved with pass@{k}: {pass_at_k_count}\")\n",
    "    print(f\"Pass@{k} success rate: {(pass_at_k_count / total_tasks) * 100:.2f}%\")\n",
    "    print(f\"Average best accuracy: {avg_best_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nAccuracy by attempt position:\")\n",
    "    for attempt, accuracy in attempt_accuracies.items():\n",
    "        print(f\"  {attempt}: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Find tasks where pass@k helped\n",
    "    helped_tasks = []\n",
    "    for result in evaluation_results:\n",
    "        if result['pass_at_k'] and result['best_solution_idx'] > 1:\n",
    "            helped_tasks.append(result)\n",
    "    \n",
    "    if helped_tasks:\n",
    "        print(f\"\\nTasks where pass@{k} helped (solution wasn't the first attempt): {len(helped_tasks)}\")\n",
    "        print(\"Examples:\")\n",
    "        for i, task in enumerate(helped_tasks[:5]):  # Show first 5 examples\n",
    "            print(f\"  {task['file_name']}: Best solution was attempt #{task['best_solution_idx']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 solutions per task for pass@3 evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Map:   0%|          | 0/1 [00:04<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[{'generated_text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHere are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\\n----------------------------------------\\n[{'input': [[8, 6], [6, 4]], 'output': [[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]}, {'input': [[7, 9], [4, 3]], 'output': [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]}]\\n----------------------------------------\\nNow, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\\n----------------------------------------\\n[{'input': [[3, 2], [7, 8]], 'output': [[]]}]\\n----------------------------------------\\nWhat is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:<|eot_id|>yntaxException.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m k_value = \u001b[32m3\u001b[39m  \u001b[38;5;66;03m# You can change this value\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m evaluation_results, dataset_with_solutions = \u001b[43mrun_passk_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_passk_evaluation\u001b[39m\u001b[34m(filtered_dataset, k, use_batch)\u001b[39m\n\u001b[32m     16\u001b[39m generation_func = generate_solutions_passk_batch\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Generate k solutions for each task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m dataset_with_solutions = \u001b[43mfiltered_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating solutions...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Evaluate pass@k performance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/datasets/arrow_dataset.py:3501\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3500\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3501\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3502\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3503\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/datasets/arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/datasets/arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mrun_passk_evaluation.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     16\u001b[39m generation_func = generate_solutions_passk_batch\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Generate k solutions for each task\u001b[39;00m\n\u001b[32m     19\u001b[39m dataset_with_solutions = filtered_dataset.map(\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mgeneration_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     21\u001b[39m     batched=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating solutions...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Evaluate pass@k performance\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mgenerate_solutions_passk_batch\u001b[39m\u001b[34m(task, k, max_new_tokens, do_sample, temperature, top_p)\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(output))\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     generated_solution = \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerated_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;28mlen\u001b[39m(prompt):]\n\u001b[32m     41\u001b[39m     generated_solutions.append(generated_solution)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mgenerated_solution_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m: sol \u001b[38;5;28;01mfor\u001b[39;00m i, sol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generated_solutions)}\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "k_value = 3  # You can change this value\n",
    "evaluation_results, dataset_with_solutions = run_passk_evaluation(filtered_dataset, k=k_value, use_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
